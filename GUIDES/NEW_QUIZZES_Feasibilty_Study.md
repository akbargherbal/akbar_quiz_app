# Expanding Quiz Types: Feasibility and Research Report

To engage hobbyist Python learners (focusing on Git, Docker, SQL, cloud CLIs, etc.), we propose extending the existing Django quiz app beyond simple multiple-choice.  The app already auto-generates questions via LLMs and parses them into XML; new formats must fit that pipeline.  Importantly, these learners value practical, context-driven questions, and the app runs on desktop and Android (mobile-friendly UI is crucial).  Below we analyze each quiz type in turn.

 ([2,000+ Free Coding & Programming Images - Pixabay](https://pixabay.com/images/search/coding/)) **Figure:** A developer practicing coding on a desktop setup (illustrative). *Extending beyond multiple-choice encourages more interactive learning experiences.* We consider True/False, Sorting/Ordering, Fill-in-the-Blank (cloze), Matching, and Code Output Prediction.  For each, we discuss **pedagogical effectiveness** for tools like Git/Docker/SQL, **LLM generation/parsing** issues, **UI usability** on desktop/mobile, and **trade-offs** or ideal use-cases.  (Note: LLM-generated quizzes often need verification – for example, one study found ChatGPT 3.5 produced only about 32% fully correct multiple-choice questions.  Structured output (XML/JSON) can aid parsing but may constrain the model.)  

## True/False Questions  
- **Pedagogical effectiveness:** True/False (binary) questions allow rapid coverage of factual concepts (e.g. “True or False: `git pull` fetches remote changes.”).  They test basic understanding of key facts and dispel misconceptions.  As one eLearning review notes, learners can answer ~3–4 T/F questions per minute, enabling broad topic coverage.  This helps retention of many small details.  However, T/F items provide low diagnostic value – each has a 50% guess rate and incorrect answers reveal little about specific misconceptions.  Thus they should target fundamental facts rather than deep reasoning.  

- **LLM generation/parsing:** Framing a statement and its truth value is technically easy for LLMs, and parsing is trivial (just “True” or “False”).  LLMs can be prompted like “Generate a true/false statement about Docker images” and output a sentence plus label.  However, care is needed: the model may produce ambiguous or trick statements.  We should enforce a clear format (e.g. XML tags `<question type="true-false">…</question>`).  Note that strict structured prompting (JSON/XML mode) can improve consistency on classification tasks like T/F, but may degrade performance on open-ended text.  In practice, a combination of free-form prompt and simple structured answer (e.g. “Question: … Answer: True”) can work.  Manual review is still recommended to catch hallucinations.

- **UI on Android/Desktop:** T/F questions are extremely mobile-friendly.  They can be rendered as two large buttons (“True” and “False”) or toggles, which are easy to tap on small screens.  This simplicity makes them accessible on Android devices.  Desktop layout is trivial as well.  No complex interaction (dragging, typing) is required.

- **Trade-offs & use-cases:** True/False is very easy to implement and parse (low development effort) but offers limited depth.  It’s ideal for quick concept checks or misconceptions (e.g. “T/F: `SELECT *` without a WHERE returns all rows.”).  However, because of guessing, a quiz of mostly T/F questions must be quite long to ensure validity.  In summary: use sparingly for basic fact review, especially when implementation speed is important.  

## Sorting/Ordering Questions  
- **Pedagogical effectiveness:** Ordering exercises ask the learner to arrange steps or items in the correct sequence.  This is well-suited for teaching procedural tasks (e.g. *“Arrange these Git commands in the order to create and merge a branch”* or *“Order the steps to build and run a Docker container”*).  Such tasks simulate real workflows and reinforce understanding of process flows.  Research on *code tracing* suggests that working through code step-by-step improves comprehension; similarly, ordering steps exercises program/project workflows.  This is very relevant for our tools: Git and Docker both have linear command sequences (init → add → commit; or pull → build → run), and SQL queries have a logical clause order (SELECT → FROM → WHERE).

- **LLM generation/parsing:** LLMs can generate ordered lists of steps or items given a scenario.  For example, prompt it with a scenario description and ask for `items_to_order`. The model may output steps in logical order, which we can then shuffle for the quiz. Parsing such lists is straightforward (e.g. XML `<step index="1">git branch dev</step>`).  However, LLMs might miss a step or include extraneous ones, so prompts should explicitly ask for *complete and distinct* steps.  Compared to true/false, this is more complex generation.  That said, classification-like parsing (assigning each item a position) is simpler.  Using a JSON/XML schema to list items helps parsing but should allow flexible language (not over-constrained).  Overall it is **moderately easy** to generate (the logic is typically in the source text) but requires careful validation (e.g. by testing the sequence against known correct flows).

- **UI on Android/Desktop:** Ordering tasks often use drag-and-drop (desktop) or tap-to-move (mobile).  On desktop, existing libraries make drag-and-drop trivial.  On Android, drag interfaces exist but can be finicky on touch; an alternative is “tap to move” or arrows to shift items.  The provided HTML example uses click-to-move code items, which could be adapted: for instance, tapping a line moves it into the sequence.  In any case, the UI should clearly show “Available” vs “Ordered” lists, and allow reordering (and going back).  Mobile screens are smaller, so the design must accommodate smaller targets and possibly vertical lists.  But sorting exercises are definitely feasible on touch devices with a good UX (e.g. enabling touch sorting or swipe).

- **Trade-offs & use-cases:** Ordering exercises have **high educational value** for procedural knowledge, but moderate implementation cost.  We must design a robust UI and ensure the LLM generates all correct steps.  They are ideal when sequence matters (e.g. Git workflows, Docker build/run, SQL query building).  However, if a process has many small sub-steps, it could overwhelm the quiz; keep lists moderately short.  In summary: **high priority** for tasks like version-control sequences or build pipelines, as they align well with hobbyists’ workflows.  

## Fill-in-the-Blank (Cloze) Questions  
- **Pedagogical effectiveness:** Cloze questions (“fill the missing word/phrase”) force active recall, which strengthens memory of specific terminology and syntax.  For example, we might blank out a keyword in a code snippet (`git ______ origin main`) or an SQL clause (`SELECT * FROM users _____ country = 'X';`).  This format is great for learning commands, options, or keyword vocabulary.  In language education, cloze tasks are known to improve comprehension and retention (learners must reconstruct the idea in context).  An LLM-based study on vocabulary found that automatically generated cloze items can be effective: about 75% of GPT-3.5–generated sentence stems were well-formed, and ~67% of the chosen word options were suitable.  By analogy, well-designed cloze questions in a programming context can enhance recall of syntax and command names.

- **LLM generation/parsing:** Generating cloze items is more challenging.  The LLM must produce a coherent sentence or code snippet with an appropriate blank and plausible distractors (if multiple-choice) or expecting a specific answer.  One approach is to ask the LLM to identify key terms in a paragraph and replace them with blanks.  In practice, we can have it output something like `<blank> = query(...)  -- fill this blank`.  The output must be parsed carefully.  Unlike true/false, answers are free text, so we’d either restrict them (e.g. multiple-choice blanks with given options) or implement fuzzy matching.  The study above suggests LLMs do a reasonably good job but not perfect.  We should prompt the model to produce clear placeholders (e.g. using a special token for blanks) and validate them.  Parsing an XML or JSON structure for blanks is possible, but beware of format errors.  

- **UI on Android/Desktop:** On desktop, fill-blank questions can appear as a sentence or code with text input fields.  On mobile, typing code or technical terms can be error-prone (small keyboard, auto-correct issues).  To improve usability on Android, consider using autocomplete or drop-downs for the blank.  For example, the blank could be a selection among likely choices (like a mini-MCQ), or the input could restrict allowed characters.  If expecting code (e.g. a command name), a monospace input field helps.  Overall it’s straightforward to display, but input accuracy is a concern for touch keyboards.

- **Trade-offs & use-cases:** Cloze has **high pedagogical value** (active recall) but **higher complexity**.  We must handle answer validation (synonyms or case differences) and generation quality.  Use cases include testing knowledge of syntax (e.g. SQL clauses, command flags, Dockerfile keywords) or code fragments.  It’s less immediate than T/F but more engaging.  In terms of effort vs. benefit, cloze is worth implementing early if we can limit it to target-specific blanks (for example, provide the first letter or context).  Automation can help here (the LLM can even suggest distractor words), but manual curation will likely be needed to ensure clarity.  

## Matching Questions  
- **Pedagogical effectiveness:** Matching quizzes ask learners to pair related items (e.g. “match the Git command to its description,” or “match Docker commands to their flags”).  This format promotes associative learning and helps reinforce vocabulary by linking terms with concepts.  For technical learners, matching is useful for synonyms or tool-command mappings (e.g. matching cloud CLI commands with their actions).  It’s also a nice way to cover multiple concepts in one question.  However, matching tends to test recognition rather than deep reasoning, and with many pairs it can feel like busy work.  Still, it’s a useful variety that can make review more engaging.

- **LLM generation/parsing:** Having an LLM produce matching pairs is tricky.  You’d need it to output two lists (left items and right items) with clear one-to-one relationships.  For instance, prompt: “List 4 Docker commands and their descriptions.”  The output could be parsed into XML pairs.  The challenge is ensuring each left item uniquely matches one right item, and the output format must be strictly controlled.  A structured prompt (e.g. JSON list of objects `{term:…, definition:…}`) could help, but again, overly strict JSON mode can reduce naturalness.  We should probably generate more items than needed and then format them properly, or use a fixed template for output.  In summary, generation is **moderately difficult** – the LLM can list terms and definitions, but formatting and correctness checks are essential. 

- **UI on Android/Desktop:** Matching is among the less mobile-friendly formats.  On desktop, it’s usually displayed as two columns with drag lines or dropdowns.  On mobile, drawing lines with a finger is not practical, so alternatives are needed: for example, tapping an item then tapping its match, or using dropdown menus for each left item.  Either way, the UI is more complex to implement.  Clarity is key: the pairs should be numbered or lettered to reduce confusion.  Many mobile quiz apps solve this by converting matching into pair-selection questions (tap one item, then its match).  It can be done, but the UI/UX work is nontrivial.

- **Trade-offs & use-cases:** Matching is useful for reinforcing terminology (e.g. tool names vs. functions).  Its **educational value** is moderate—better than pure memorization (like T/F) but not as deep as problem-solving.  It’s also more work to build: we need a custom UI and reliable pair generation.  Thus, matching might be a mid-term priority.  It’s ideal for vocabulary and concept mapping (e.g. match cloud CLI flags to their meaning), but probably shouldn’t be the very first format added unless we specifically need it. 

## Code Output Prediction Questions  
 ([2,000+ Free Coding & Programming Images - Pixabay](https://pixabay.com/images/search/coding/)) **Figure:** Example code listing. *Learners predict the output or result of code or commands.* This is essentially a code-tracing exercise.  

- **Pedagogical effectiveness:** Extremely high for programming.  Predicting output forces learners to mentally execute code line-by-line, which builds deep understanding.  In computing education literature, code tracing (reading and simulating code) is known to greatly improve comprehension.  For Python hobbyists, this directly targets debugging skills and logical thinking.  For example, showing a short Python loop or a SQL query and asking “what does it print/return?” checks that the student truly understands syntax and semantics.  Similarly, we could ask for the result of a `docker run` command or the output of a `gcloud` command on sample input.  This format aligns well with intermediate learners and practical tools.

- **LLM generation/parsing:** This is challenging to automate fully.  We need the LLM to produce correct code snippets **and** their outputs.  While LLMs can generate code, they sometimes make mistakes.  One approach is to have the LLM generate the question text (code snippet) and **then actually execute the code** in a sandbox to get the true output, rather than trusting the LLM’s answer.  If we stick to LLM-only, the prompt must clearly specify the code and ask *only* for output.  Parsing the answer can be done in JSON/XML (e.g. `<output>…</output>`), but even then, the model’s output might need normalization (extra text, formatting).  Because this is effectively a classification output, imposing a strict answer format actually helps (e.g. “provide only the output, no explanation”).  In summary, **generation is difficult**: it may be safer to have the LLM generate just the code (with intended behavior) and compute the output externally.  However, if using LLM for both, careful validation is mandatory.

- **UI on Android/Desktop:** Displaying code snippets and receiving answers is quite doable on both platforms.  We’d show a monospace block of code or command, then either let the user type or select the output.  Typing answers (especially exact output) on mobile can be a bit cumbersome, but since outputs are often short (numbers, lines of text), it’s not too bad.  For convenience, we could offer multiple choice answers for code output (to avoid typing), or provide a “copy code” button.  Touch targets (like radio buttons) work well.  Overall, code-output questions are **mobile-friendly** aside from potential small input issues, but the interface is standard (display text, input field).

- **Trade-offs & use-cases:** These questions have the **highest educational value** in terms of real skill-building.  They directly train debugging and comprehension.  The downside is implementation complexity: verifying outputs (either by running code or carefully parsing LLM output) and formatting code text.  They also require that learners already know syntax (which fits an intermediate audience).  Given their value, they should be **high priority** despite the effort.  

## Recommendations and Prioritization

Based on the above, we suggest the following priority order (high priority first):

1. **Sorting/Ordering** – Offers a good balance of educational impact (procedural understanding) and feasibility. Many programming tasks involve sequences (e.g. Git workflows, Docker steps, SQL clauses). The UI is moderately complex but achievable, and LLM generation is straightforward if carefully prompted. *Implementation effort:* moderate. *Educational value:* high.  
2. **Code Output Prediction** – Highest learning value (code comprehension), but also high effort. We recommend including this early (possibly as a premium feature), since it directly trains coding skills. LLM can help generate questions, but verifying answers (e.g. by executing code) is recommended. *Effort:* high. *Value:* very high.  
3. **Fill-in-the-Blank (Cloze)** – High value for recall practice with moderate effort. Generating the blanks will need careful prompts and some manual curation, but is very engaging.  Good for reinforcing syntax and commands. *Effort:* moderate. *Value:* high.  
4. **Matching** – Useful for terminology practice, but lower impact than the above. UI work (especially for mobile) is significant, and LLM generation needs pairing logic. We recommend this after ordering and cloze features are solid. *Effort:* moderate-high. *Value:* moderate.  
5. **True/False** – Easiest to implement (low effort) but lowest value for deep learning. It is useful for quick sanity checks of factual knowledge. We suggest using T/F selectively (e.g. for quick quizzes or concept checks) but not as a main focus.  If resources are tight, this can be added last. *Effort:* low. *Value:* low.

Overall, we advise focusing first on interactive, procedural formats (ordering and code-tracing), as these align best with hobbyist learners’ desire for applied practice.  Incorporate structured prompts for LLMs (to ease parsing) but allow some flexibility in wording.  In all cases, plan to review generated content: as one study warns, LLM-generated quizzes often contain mistakes without oversight. With thoughtful design, these new formats can make the app both more engaging and more effective for intermediate Python practitioners on mobile and desktop.  

**Sources:** Findings draw on educational research on quiz formats and recent studies of LLM-generated assessments, alongside UI/UX best practices for touch interfaces.  

