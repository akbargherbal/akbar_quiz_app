{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b31c4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import regex as re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90c1c9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('./400QA_Understanding_QuizMaster_Codebase.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ab7cbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3de0917",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "list_pkl = []\n",
    "for root, dirs, files in os.walk('.'):\n",
    "    for file in files:\n",
    "        if file.endswith('.pkl'):\n",
    "            list_pkl.append(os.path.join(root, file).replace(os.sep, '/'))\n",
    "\n",
    "filter_out = ['appender', 'log4j', 'Singleton', 'SLF4J', ' java ']\n",
    "\n",
    "# filter out questions containing the any of the above keyworkds\n",
    "\n",
    "for pkl in list_pkl:\n",
    "    df = pd.read_pickle(pkl)\n",
    "    df = df['question_text'].apply(lambda x: x.lower()) # modify this line to remove any of the filter_out keywords words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2a13373",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_out = [i for i in filter_out if i.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50995bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00268531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 pickle files to process.\n",
      "Filtering out questions containing: ['appender', 'log4j', 'singleton', 'slf4j', ' java ']\n",
      "--- IMPORTANT: Original files will be overwritten! ---\n",
      "------------------------------\n",
      "Processing: ./400QA_Understanding_QuizMaster_Codebase.pkl\n",
      "  No questions matched the filter keywords.\n",
      "  Summary: Original rows: 395, Filtered rows: 395, Rows removed: 0\n",
      "  Overwrote ./400QA_Understanding_QuizMaster_Codebase.pkl with filtered data.\n",
      "------------------------------\n",
      "Processing: ./70QA_QUIZ_BANK_02.pkl\n",
      "  No questions matched the filter keywords.\n",
      "  Summary: Original rows: 70, Filtered rows: 70, Rows removed: 0\n",
      "  Overwrote ./70QA_QUIZ_BANK_02.pkl with filtered data.\n",
      "------------------------------\n",
      "Processing: ./83QA_GCP_ESSENTIALS_QUIZ_BANK.pkl\n",
      "  No questions matched the filter keywords.\n",
      "  Summary: Original rows: 83, Filtered rows: 83, Rows removed: 0\n",
      "  Overwrote ./83QA_GCP_ESSENTIALS_QUIZ_BANK.pkl with filtered data.\n",
      "------------------------------\n",
      "Processing complete. All specified files have been filtered and overwritten.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "# import pickle # Not needed if using pd.read_pickle\n",
    "\n",
    "list_pkl = []\n",
    "# Make sure '.' is the correct starting directory, or specify another path\n",
    "start_dir = '.' \n",
    "for root, dirs, files in os.walk(start_dir):\n",
    "    for file in files:\n",
    "        if file.endswith('.pkl'):\n",
    "            # Use forward slashes for consistency if desired\n",
    "            list_pkl.append(os.path.join(root, file).replace(os.sep, '/')) \n",
    "\n",
    "# Ensure filter keywords are lowercase for case-insensitive matching\n",
    "filter_out = ['appender', 'log4j', 'singleton', 'slf4j', ' java '] \n",
    "# Create a regex pattern joining keywords with OR '|'\n",
    "# This allows checking for any of the keywords in one pass\n",
    "filter_out_pattern = '|'.join(filter_out)\n",
    "\n",
    "print(f\"Found {len(list_pkl)} pickle files to process.\")\n",
    "print(f\"Filtering out questions containing: {filter_out}\")\n",
    "print(\"--- IMPORTANT: Original files will be overwritten! ---\") # Added warning\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# filtered_results = {} # Not strictly needed if overwriting\n",
    "\n",
    "for pkl_path in list_pkl:\n",
    "    print(f\"Processing: {pkl_path}\")\n",
    "    try:\n",
    "        df = pd.read_pickle(pkl_path)\n",
    "\n",
    "        # Check if the expected column exists\n",
    "        if 'question_text' not in df.columns:\n",
    "            print(f\"  Warning: 'question_text' column not found in {pkl_path}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        # --- Identify rows to filter out ---\n",
    "        # Ensure the column is treated as string and handle potential NaN values\n",
    "        # Create a boolean mask: True if the lowercase text contains any keyword\n",
    "        mask_contains_keyword = df['question_text'].astype(str).str.lower().str.contains(\n",
    "            filter_out_pattern, \n",
    "            regex=True, \n",
    "            na=False # Treat NaN as not containing the keywords\n",
    "        )\n",
    "        \n",
    "        # --- Print the questions being filtered out ---\n",
    "        df_to_remove = df[mask_contains_keyword]\n",
    "        removed_count = len(df_to_remove)\n",
    "\n",
    "        if removed_count > 0:\n",
    "            print(f\"  --- Questions Filtered Out ({removed_count}) ---\")\n",
    "            for index, row in df_to_remove.iterrows():\n",
    "                # Print the original question text\n",
    "                print(f\"    - {row['question_text']}\") \n",
    "            print(f\"  --- End of Filtered Out Questions ---\")\n",
    "        else:\n",
    "            print(\"  No questions matched the filter keywords.\")\n",
    "\n",
    "        # --- Filter the DataFrame ---\n",
    "        # Keep rows where the mask is False (i.e., keyword NOT found)\n",
    "        df_filtered = df[~mask_contains_keyword]\n",
    "        \n",
    "        original_count = len(df)\n",
    "        filtered_count = len(df_filtered)\n",
    "        \n",
    "        print(f\"  Summary: Original rows: {original_count}, Filtered rows: {filtered_count}, Rows removed: {removed_count}\")\n",
    "        \n",
    "        # --- Overwrite the original file ---\n",
    "        df_filtered.to_pickle(pkl_path) \n",
    "        print(f\"  Overwrote {pkl_path} with filtered data.\")\n",
    "        print(\"-\" * 30) # Separator between files\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"  Error: File not found {pkl_path}\")\n",
    "        print(\"-\" * 30) \n",
    "    except Exception as e:\n",
    "        print(f\"  Error processing file {pkl_path}: {e}\")\n",
    "        print(\"-\" * 30) \n",
    "\n",
    "print(\"Processing complete. All specified files have been filtered and overwritten.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7beb2d89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
